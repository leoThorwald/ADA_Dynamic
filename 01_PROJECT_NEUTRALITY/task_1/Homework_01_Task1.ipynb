{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd67335b-66f3-472b-a006-35739072fcfe",
   "metadata": {},
   "source": [
    "## Task 1 : What's in an average anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444b8296-4e1f-46da-b164-bee4d3cdbe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:07.105929Z",
     "iopub.status.busy": "2024-10-05T07:39:07.103676Z",
     "iopub.status.idle": "2024-10-05T07:39:09.317147Z",
     "shell.execute_reply": "2024-10-05T07:39:09.315717Z",
     "shell.execute_reply.started": "2024-10-05T07:39:07.105840Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479b07d-d8b1-4364-a34b-1deeb3c83975",
   "metadata": {},
   "source": [
    "### 1.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97fa2cb2-0003-4f27-8359-ff8bcb331678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.320166Z",
     "iopub.status.busy": "2024-10-05T07:39:09.319505Z",
     "iopub.status.idle": "2024-10-05T07:39:09.507843Z",
     "shell.execute_reply": "2024-10-05T07:39:09.506967Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.320130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test size: (14042, 8)\n",
      "df_x size: (13882, 2)\n",
      "df_y size: (13978, 2)\n",
      "df_z size: (13923, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load the dataframes\n",
    "df_test=pd.read_csv('mmlu_data/test.csv')\n",
    "df_x=pd.read_csv('lm_scores/lm_X.csv')\n",
    "df_y=pd.read_csv('lm_scores/lm_Y.csv')\n",
    "df_z=pd.read_csv('lm_scores/lm_Z.csv')\n",
    "\n",
    "#Print the sizes for each dataframe\n",
    "print('df_test size:',df_test.shape)\n",
    "print('df_x size:',df_x.shape)\n",
    "print('df_y size:',df_y.shape)\n",
    "print('df_z size:',df_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cdb74-d6ec-42e6-9f10-5d57b336409f",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e905daab-dfe9-4b5a-b08f-b73e4264fa56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.509945Z",
     "iopub.status.busy": "2024-10-05T07:39:09.509233Z",
     "iopub.status.idle": "2024-10-05T07:39:09.531908Z",
     "shell.execute_reply": "2024-10-05T07:39:09.530424Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.509894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result in language model X:\n",
      " result\n",
      "A                                                                                    2733\n",
      "A                                                                                    1657\n",
      "B                                                                                    1412\n",
      "Answer: A                                                                            1398\n",
      "C                                                                                    1134\n",
      "                                                                                     ... \n",
      "(–3, 2), so the answer is C                                                             1\n",
      "sweet, so the answer is B                                                               1\n",
      "remembering how to tie a tie, so the answer is B                                        1\n",
      "anger, so the answer is A                                                               1\n",
      "creating insurmountable obstacles to the founding of factions, so the answer is A       1\n",
      "Name: count, Length: 145, dtype: int64\n",
      "\n",
      "Result in language model Y:\n",
      " result\n",
      "D                                                                                                                 2894\n",
      "Answer: D                                                                                                         1718\n",
      "C                                                                                                                 1701\n",
      "B                                                                                                                 1240\n",
      "D                                                                                                                 1145\n",
      "                                                                                                                  ... \n",
      "4, so the answer is D                                                                                                1\n",
      "FindName ([\"Andrea\", \"Ben\", \"Chris\"], \"Ben\"), so the answer is C                                                     1\n",
      "had some degree of a nonaggression agreement since at least 1926, so the answer is A                                 1\n",
      "women held considerable power and responsibility within the domestic sphere of social life, so the answer is C       1\n",
      "ADP + P → ATP, so the answer is D                                                                                    1\n",
      "Name: count, Length: 141, dtype: int64\n",
      "\n",
      "Result in language model Z:\n",
      " result\n",
      "D                                                                                                                 2894\n",
      "Answer: D                                                                                                         1718\n",
      "C                                                                                                                 1701\n",
      "B                                                                                                                 1240\n",
      "D                                                                                                                 1145\n",
      "                                                                                                                  ... \n",
      "4, so the answer is D                                                                                                1\n",
      "FindName ([\"Andrea\", \"Ben\", \"Chris\"], \"Ben\"), so the answer is C                                                     1\n",
      "had some degree of a nonaggression agreement since at least 1926, so the answer is A                                 1\n",
      "women held considerable power and responsibility within the domestic sphere of social life, so the answer is C       1\n",
      "ADP + P → ATP, so the answer is D                                                                                    1\n",
      "Name: count, Length: 141, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "#Use a 'value_counts()' operation for each score dataframe\n",
    "df_x_counts=df_x['result'].value_counts()\n",
    "df_y_counts=df_y['result'].value_counts()\n",
    "df_z_counts=df_y['result'].value_counts()\n",
    "\n",
    "#Print the result for each value counts\n",
    "print(\"Result in language model X:\\n\",df_x_counts)\n",
    "print(\"\\nResult in language model Y:\\n\",df_y_counts)\n",
    "print(\"\\nResult in language model Z:\\n\",df_z_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e57406-4cdb-4df6-bcc0-4f438125715a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.534653Z",
     "iopub.status.busy": "2024-10-05T07:39:09.534343Z",
     "iopub.status.idle": "2024-10-05T07:39:09.624707Z",
     "shell.execute_reply": "2024-10-05T07:39:09.623058Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.534625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected results in language model X:\n",
      " result\n",
      "A                                                                                                     1657\n",
      "Answer: A                                                                                             1398\n",
      "B                                                                                                      793\n",
      "Answer: B                                                                                              760\n",
      "C                                                                                                      622\n",
      "Answer: D                                                                                              613\n",
      "D                                                                                                      596\n",
      "Answer: C                                                                                              594\n",
      "Not Sure                                                                                                73\n",
      "None of the above                                                                                       69\n",
      "Wrong, Wrong, so the answer is A                                                                         4\n",
      "Vitamin D, so the answer is B                                                                            1\n",
      "Folate, vitamins B6 and B12, so the answer is D                                                          1\n",
      "purely physical and natural, so the answer is B                                                          1\n",
      "logical nominalism, so the answer is A                                                                   1\n",
      "enlightened egoism., so the answer is A                                                                  1\n",
      "not recover, because the dairy's negligence only caused mental disturbance., so the answer is C          1\n",
      "food surpluses, specialists, urban settlements, and a system of record keeping, so the answer is A       1\n",
      "Not wrong, Not wrong, so the answer is D                                                                 1\n",
      "philosophy and religion., so the answer is B                                                             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unexpected results in language model Y:\n",
      " result\n",
      "Answer: D                                         1718\n",
      "D                                                 1145\n",
      "Answer: C                                          907\n",
      "Answer: B                                          726\n",
      "C                                                  634\n",
      "Answer: A                                          597\n",
      "B                                                  553\n",
      "A                                                  428\n",
      "Not Sure                                            86\n",
      "None of the above                                   66\n",
      "(i), (ii), (iii), and (iv), so the answer is D       2\n",
      "Not wrong, Wrong, so the answer is C                 2\n",
      "Not wrong, Not wrong, so the answer is D             2\n",
      "Denver boot, so the answer is B                      1\n",
      "legs, so the answer is D                             1\n",
      "fencing, so the answer is B                          1\n",
      "Broadcast media., so the answer is A                 1\n",
      "Environmental scanning, so the answer is A           1\n",
      "5 cents, so the answer is A                          1\n",
      "gaggle, so the answer is A                           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unexpected results in language model Z:\n",
      " result\n",
      "Answer: D                                         777\n",
      "Answer: C                                         748\n",
      "Answer: B                                         712\n",
      "Answer: A                                         687\n",
      "C                                                 316\n",
      "D                                                 314\n",
      "B                                                 285\n",
      "A                                                 279\n",
      "Not Sure                                          125\n",
      "None of the above                                 119\n",
      "Not wrong, Not wrong, so the answer is D           10\n",
      "Wrong, Wrong, so the answer is A                    9\n",
      "Wrong, Not wrong, so the answer is B                9\n",
      "Not wrong, Wrong, so the answer is C                8\n",
      "Contradictory, so the answer is B                   2\n",
      "II only, so the answer is B                         2\n",
      "all of the above, so the answer is D                2\n",
      "Vitamin D, so the answer is B                       1\n",
      "Increased risk of fracture, so the answer is B      1\n",
      "All of the above, so the answer is D                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "#Directly show unexpected values for each dataframe\n",
    "expected_answers=['A','B','C','D']\n",
    "df_x_unexpected=df_x[~df_x['result'].isin(expected_answers)]['result'].value_counts()\n",
    "df_y_unexpected=df_y[~df_y['result'].isin(expected_answers)]['result'].value_counts()\n",
    "df_z_unexpected=df_z[~df_z['result'].isin(expected_answers)]['result'].value_counts()\n",
    "\n",
    "# Print rows where 'result' column is unexpected in each dataframe\n",
    "print(\"Unexpected results in language model X:\\n\",df_x_unexpected.head(20))\n",
    "print(\"\\nUnexpected results in language model Y:\\n\",df_y_unexpected.head(20))\n",
    "print(\"\\nUnexpected results in language model Z:\\n\",df_z_unexpected.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13bf5a-6b70-41f4-baf0-c1963c96320c",
   "metadata": {},
   "source": [
    "**#B./Discuss:/**<br>\n",
    "According to the above:<br>\n",
    "Expected format: The answer contains only one letter, one of \"A\",\"B\",\"C\" or \"D\".<br>\n",
    "Unexpected formats:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1. Answers with extra spaces, such as \"D \", \"B \".<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2. Answers that contain extra or complex explanations, such as \"Answer: B\", \"food surpluses, specialists, urban settlements, and a system of record keeping, so the answer is A\".<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3. No exact answer, such as \"None of the above\", \"Not Sure\".<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4. Answers that contradict their explanations, such as \"Wrong, Wrong, so the answer is A\", \"Not wrong, Not wrong, so the answer is D\".<br>\n",
    "These unexpected formats affect the consistency of the formatting and make the answers more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed38124-c64c-4777-90ba-56cf81216601",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee904d78-5c00-42dd-895a-e1733e7e6e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.626686Z",
     "iopub.status.busy": "2024-10-05T07:39:09.626273Z",
     "iopub.status.idle": "2024-10-05T07:39:09.677517Z",
     "shell.execute_reply": "2024-10-05T07:39:09.676009Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.626655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results in language model X:\n",
      "        question_id     result\n",
      "0                0          B\n",
      "1                1          C\n",
      "2                2         D \n",
      "3                3         B \n",
      "4                4  Answer: B\n",
      "...            ...        ...\n",
      "13877        14037         A \n",
      "13878        14038          A\n",
      "13879        14039          B\n",
      "13880        14040          B\n",
      "13881        14041  Answer: A\n",
      "\n",
      "[13679 rows x 2 columns]\n",
      "\n",
      "Filtered results in language model Y:\n",
      "        question_id     result\n",
      "0                0  Answer: D\n",
      "1                1          D\n",
      "2                2  Answer: D\n",
      "3                3        NaN\n",
      "4                4          D\n",
      "...            ...        ...\n",
      "13973        14037         C \n",
      "13974        14038          D\n",
      "13975        14039  Answer: D\n",
      "13976        14040          B\n",
      "13977        14041         D \n",
      "\n",
      "[13782 rows x 2 columns]\n",
      "\n",
      "Filtered results in language model Z:\n",
      "        question_id     result\n",
      "0                0          B\n",
      "1                1  Answer: B\n",
      "2                2          C\n",
      "3                3         B \n",
      "4                4          B\n",
      "...            ...        ...\n",
      "13918        14037          A\n",
      "13919        14038          A\n",
      "13920        14039          B\n",
      "13921        14040         B \n",
      "13922        14041          A\n",
      "\n",
      "[13223 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "#Filter the answers in the 'result' column that are less than 10 characters long and create a deep copy\n",
    "df_x_filtered=df_x[df_x['result'].apply(lambda x: len(str(x))<10)].copy(deep=True)\n",
    "df_y_filtered=df_y[df_y['result'].apply(lambda y: len(str(y))<10)].copy(deep=True)\n",
    "df_z_filtered=df_z[df_z['result'].apply(lambda z: len(str(z))<10)].copy(deep=True)\n",
    "\n",
    "#Print the result\n",
    "print('Filtered results in language model X:\\n',df_x_filtered)\n",
    "print('\\nFiltered results in language model Y:\\n',df_y_filtered)\n",
    "print('\\nFiltered results in language model Z:\\n',df_z_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efcb89b-33f4-429e-a66b-9222168b4c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.679570Z",
     "iopub.status.busy": "2024-10-05T07:39:09.679212Z",
     "iopub.status.idle": "2024-10-05T07:39:09.725229Z",
     "shell.execute_reply": "2024-10-05T07:39:09.722992Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.679539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x_cleaned distribution:\n",
      " result\n",
      "A    4390\n",
      "B    2205\n",
      "C    1756\n",
      "D    1720\n",
      "Name: count, dtype: int64\n",
      "\n",
      "df_y_cleaned distribution:\n",
      " result\n",
      "D    4039\n",
      "C    2335\n",
      "B    1793\n",
      "A    1436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "df_z_cleaned distribution:\n",
      " result\n",
      "D    2571\n",
      "C    2507\n",
      "B    2412\n",
      "A    2339\n",
      "Name: count, dtype: int64\n",
      "df_x_cleaned size: (10071, 2)\n",
      "df_y_cleaned size: (9603, 2)\n",
      "df_z_cleaned size: (9829, 2)\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "# Define the cleaning function\n",
    "def clean_answer(s, pattern='-'):\n",
    "    return str(s).replace(pattern, '').strip()\n",
    "\n",
    "# Apply the cleaning function to the 'result' column in the three filtered dataframes\n",
    "df_x_filtered['result']=df_x_filtered['result'].apply(lambda x: clean_answer(x,'-'))\n",
    "df_y_filtered['result']=df_y_filtered['result'].apply(lambda x: clean_answer(x,'-'))\n",
    "df_z_filtered['result']=df_z_filtered['result'].apply(lambda x: clean_answer(x,'-'))\n",
    "\n",
    "# Make sure all remaining answers are one of A, B, C, or D\n",
    "df_x_cleaned=df_x_filtered[df_x_filtered['result'].isin(expected_answers)]\n",
    "df_y_cleaned=df_y_filtered[df_y_filtered['result'].isin(expected_answers)]\n",
    "df_z_cleaned=df_z_filtered[df_z_filtered['result'].isin(expected_answers)]\n",
    "\n",
    "# Print the the remaining answer distribution to verify\n",
    "print(\"df_x_cleaned distribution:\\n\",df_x_cleaned['result'].value_counts())\n",
    "print(\"\\ndf_y_cleaned distribution:\\n\", df_y_cleaned['result'].value_counts())\n",
    "print(\"\\ndf_z_cleaned distribution:\\n\", df_z_cleaned['result'].value_counts())\n",
    "print('df_x_cleaned size:',df_x_cleaned.shape)\n",
    "print('df_y_cleaned size:',df_y_cleaned.shape)\n",
    "print('df_z_cleaned size:',df_z_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a79b28-6b64-4752-a6b2-85879c33b7b9",
   "metadata": {},
   "source": [
    "**#C./Discuss:/**<br>\n",
    "In the original data (1.1), we can see that the three language model data frames (lm_x, lm_y, and lm_z) have 13882, 13978, and 13923 rows respectively, but after filtering, only 10071, 9603, and 9829 rows are left. Nearly 30% of the data is deleted during the filtering process.<br>\n",
    "This is a huge amount of data, and its loss may cause some valid answers to be deleted, affecting the representativeness and credibility of the data, and further affecting the subsequent analysis or model training results. On the other hand, this also means that a large amount of data contains invalid or unexpected formats, indicating that this is not a sufficiently convincing experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330387f2-2521-4e0f-85c4-4ca2e9be20db",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966dccc8-ae23-41f1-8391-d463e0fdeccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.729539Z",
     "iopub.status.busy": "2024-10-05T07:39:09.728596Z",
     "iopub.status.idle": "2024-10-05T07:39:09.769363Z",
     "shell.execute_reply": "2024-10-05T07:39:09.767864Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.729463Z"
    }
   },
   "outputs": [],
   "source": [
    "#A\n",
    "#Use an inner join operation with the \"df_test\" dataframe on the question_id column for each of the language model score data frames\n",
    "df_x_joined=df_test.merge(df_x_cleaned, on='question_id', how='inner')\n",
    "df_y_joined=df_test.merge(df_y_cleaned, on='question_id', how='inner')\n",
    "df_z_joined=df_test.merge(df_z_cleaned, on='question_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fdd002-992a-4cfe-b1b0-7ed8afdc970a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:09.772041Z",
     "iopub.status.busy": "2024-10-05T07:39:09.771678Z",
     "iopub.status.idle": "2024-10-05T07:39:10.102940Z",
     "shell.execute_reply": "2024-10-05T07:39:10.101213Z",
     "shell.execute_reply.started": "2024-10-05T07:39:09.772010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of language model x: 0.7679\n",
      "\n",
      "The average score of language model y: 0.7452\n",
      "\n",
      "The average score of language model z: 0.6606\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "#Define a function to check if the model's predictions are correct: \n",
    "#Assume that return 1 if the model's answer in \"result\" column is the same as the expected answer in column \"answer\", otherwise return 0\n",
    "def check_correct(row):\n",
    "    return 1 if row['result']==row['answer'] else 0\n",
    "\n",
    "#Apply the function to the three language model score data frames and add a 'correct' column.\n",
    "df_x_joined['correct']=df_x_joined.apply(check_correct,axis=1)\n",
    "df_y_joined['correct']=df_y_joined.apply(check_correct,axis=1)\n",
    "df_z_joined['correct']=df_z_joined.apply(check_correct,axis=1)\n",
    "\n",
    "#Calculate the average score for each model to four decimal places\n",
    "mean_score_x=round(df_x_joined['correct'].mean(),4)\n",
    "mean_score_y=round(df_y_joined['correct'].mean(),4)\n",
    "mean_score_z=round(df_z_joined['correct'].mean(),4)\n",
    "\n",
    "#Print the average score for each model\n",
    "print(\"The average score of language model x:\",mean_score_x)\n",
    "print(\"\\nThe average score of language model y:\",mean_score_y)\n",
    "print(\"\\nThe average score of language model z:\",mean_score_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4039314-5cd8-4b12-9a58-64a7e7720eb9",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ff89dd-fcac-4d55-a94e-8687316e49fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T07:39:10.105013Z",
     "iopub.status.busy": "2024-10-05T07:39:10.104692Z",
     "iopub.status.idle": "2024-10-05T07:39:10.127737Z",
     "shell.execute_reply": "2024-10-05T07:39:10.126645Z",
     "shell.execute_reply.started": "2024-10-05T07:39:10.104986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are the subjects with more than 10% difference:\n",
      "\n",
      "                              lm_x  lm_y  lm_z\n",
      "subject                                       \n",
      "abstract algebra                61    73    80\n",
      "business ethics                 77    67    61\n",
      "college biology                110    89    93\n",
      "college chemistry               78    72    61\n",
      "college computer science        69    72    62\n",
      "computer security               64    75    69\n",
      "conceptual physics             178   157   155\n",
      "electrical engineering         105    90   102\n",
      "formal logic                    79    86    88\n",
      "global facts                    82    69    74\n",
      "high school biology            213   199   222\n",
      "high school european history   126   118   113\n",
      "high school physics            102    91    94\n",
      "high school us history         151   135   155\n",
      "logical fallacies              128    93   110\n",
      "machine learning                84    76    86\n",
      "management                      76    77    65\n",
      "marketing                      180   161   164\n",
      "medical genetics                76    69    68\n",
      "moral disputes                 243   222   189\n",
      "moral scenarios                554   618   618\n",
      "security studies               174   157   178\n",
      "us foreign policy               69    63    74\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "#Count the number of questions answered by each model in 57 subjects\n",
    "questions_per_subject_x=df_x_joined.groupby('subject')['question_id'].count()\n",
    "questions_per_subject_y=df_y_joined.groupby('subject')['question_id'].count()\n",
    "questions_per_subject_z=df_z_joined.groupby('subject')['question_id'].count()\n",
    "\n",
    "#Summarize the number of responses for each model in each subject and replace missing values with 0\n",
    "df_questions=pd.DataFrame({'lm_x':questions_per_subject_x,\n",
    "                           'lm_y':questions_per_subject_y,\n",
    "                           'lm_z':questions_per_subject_z,\n",
    "                          }).fillna(0)\n",
    "\n",
    "#Define a helper function to check if the relative difference between two models exceeds 10% at the same time\n",
    "def check_diff(val1,val2):\n",
    "    if val1!=0 and val2!=0:\n",
    "        diff_1=abs(val2-val1)/val1>0.1\n",
    "        diff_2=abs(val2-val1)/val2>0.1\n",
    "        return diff_1 and diff_2\n",
    "    return False\n",
    "\n",
    "#Define a main function to check the difference between each pair of models in turn.\n",
    "def large_diff(row):\n",
    "    return check_diff(row['lm_x'],row['lm_y']) or check_diff(row['lm_x'],row['lm_z']) or check_diff(row['lm_y'],row['lm_z'])\n",
    "\n",
    "#Use the diff function for Boolean indexing\n",
    "subjects_with_large_diff=df_questions[df_questions.apply(large_diff,axis=1)]\n",
    "\n",
    "#Print the subjects for which there is a more than 10% difference\n",
    "if not subjects_with_large_diff.empty:\n",
    "    print(\"There are the subjects with more than 10% difference:\\n\")\n",
    "    print(subjects_with_large_diff)\n",
    "else:\n",
    "    print(\"There is no subject with more than 10% difference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b316b69-5efb-4b33-9f0d-019e9160c3ea",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-05T09:08:52.948213Z",
     "iopub.status.busy": "2024-10-05T09:08:52.944834Z",
     "iopub.status.idle": "2024-10-05T09:08:53.097130Z",
     "shell.execute_reply": "2024-10-05T09:08:53.095161Z",
     "shell.execute_reply.started": "2024-10-05T09:08:52.948120Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121/303238862.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df_model.groupby('subject',group_keys=False).apply(lambda x: x.sample(n=max_count[x.name],replace=True))\n",
      "/tmp/ipykernel_121/303238862.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df_model.groupby('subject',group_keys=False).apply(lambda x: x.sample(n=max_count[x.name],replace=True))\n",
      "/tmp/ipykernel_121/303238862.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df_model.groupby('subject',group_keys=False).apply(lambda x: x.sample(n=max_count[x.name],replace=True))\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "#Firstly, using weight adjustment to balance the influence of each model\n",
    "#Define a function to adjust and regularize the weight of the number of answers for each model\n",
    "def count_weight(row):\n",
    "    weight_x=(1/row['lm_x']) if row['lm_x']>0 else 0\n",
    "    weight_y=(1/row['lm_y']) if row['lm_y']>0 else 0\n",
    "    weight_z=(1/row['lm_z']) if row['lm_z']>0 else 0\n",
    "    total_weight=weight_x+weight_y+weight_z\n",
    "    return pd.Series([weight_x/total_weight,weight_y/total_weight,weight_z/total_weight])\n",
    "\n",
    "#Calculate model weights for each subject\n",
    "df_weights=df_questions.apply(count_weight,axis=1)\n",
    "df_weights.columns=['weight_x','weight_y','weight_z']\n",
    "\n",
    "#Apply weights to the answer of each model\n",
    "df_x_weighted=df_x_joined.merge(df_weights['weight_x'],on='subject')\n",
    "df_y_weighted=df_y_joined.merge(df_weights['weight_y'],on='subject')\n",
    "df_z_weighted=df_z_joined.merge(df_weights['weight_z'],on='subject')\n",
    "\n",
    "#Secondly, using oversampling in extremely imbalanced topics to reduce the gap in answers between models.\n",
    "#Define a oversampling function to expand the model data with fewer answers\n",
    "def oversample(df_model,max_count):\n",
    "    return df_model.groupby('subject',group_keys=False).apply(lambda x: x.sample(n=max_count[x.name],replace=True))\n",
    "\n",
    "#Find the maximum number of answered questions per subject\n",
    "max_questions_per_subject=df_questions.max(axis=1)\n",
    "\n",
    "#Oversampling models with fewer answers\n",
    "df_x_oversampling=oversample(df_x_weighted,max_questions_per_subject)\n",
    "df_y_oversampling=oversample(df_y_weighted,max_questions_per_subject)\n",
    "df_z_oversampling=oversample(df_z_weighted,max_questions_per_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1eae7a64-5ef0-432a-9cfa-ea9e915e1b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T09:16:38.585874Z",
     "iopub.status.busy": "2024-10-05T09:16:38.582026Z",
     "iopub.status.idle": "2024-10-05T09:16:38.616876Z",
     "shell.execute_reply": "2024-10-05T09:16:38.613654Z",
     "shell.execute_reply.started": "2024-10-05T09:16:38.585733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebalanced accuracy of model X:0.7655\n",
      "Rebalanced accuracy of model Y:0.7403\n",
      "Rebalanced accuracy of model Z:0.6627\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "#Define a function to calculate accuracy\n",
    "def accuracy(df_rebalanced,weight_column):\n",
    "      return np.average(df_rebalanced['correct'],weights=df_rebalanced[weight_column])\n",
    "\n",
    "#Calculate and the rebalanced accuracy of each model and control the display format to 4 decimal places\n",
    "rebalanced_accuracy_x=f\"{accuracy(df_x_oversampling,'weight_x'):.4f}\"\n",
    "rebalanced_accuracy_y=f\"{accuracy(df_y_oversampling,'weight_y'):.4f}\"\n",
    "rebalanced_accuracy_z=f\"{accuracy(df_z_oversampling,'weight_z'):.4f}\"\n",
    "\n",
    "#Print the rebalanced accuracy of each model\n",
    "print(f\"Rebalanced accuracy of model X:{rebalanced_accuracy_x}\")\n",
    "print(f\"Rebalanced accuracy of model Y:{rebalanced_accuracy_y}\")\n",
    "print(f\"Rebalanced accuracy of model Z:{rebalanced_accuracy_z}\")"
   ]
  },
  
